name: Update CSV File

on:
  schedule:
    - cron: '0 * * * *'  # This runs the workflow every hour
  push:
    branches:
      - main

jobs:
  scrape:
    strategy:
      matrix:
        # offset: [0, 200, 400, 800, 1000, 2500, 3000]  # Define your offsets here
        offset: [0, 200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000, 2200, 2400, 2600, 2800, 3000]

    runs-on: ubuntu-latest

    steps:
      - name: Checkout repository
        uses: actions/checkout@v2

      - name: Set up Python
        uses: actions/setup-python@v2
        with:
          python-version: 3.x

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install requests beautifulsoup4 pandas

      - name: Run scraper and save to temp file
        run: |
          python scraper.py ${{ matrix.offset }}
      - name: List files
        run: ls -l


  consolidate:
    needs: scrape  # This ensures it runs after all scrape jobs are done
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v2

      - name: Consolidate results
        run: |
          # echo "" > data.csv  # Ensure file is empty before appending
          for offset in 0 200 400 600 800 1000 1200 1400 1600 1800 2000 2200 2400 2600 2800 3000; do
            cat temp_$offset.csv >> final_results.csv
          done

      - name: Commit and push consolidated results
        env:
          PAT_TOKEN: ${{ secrets.PAT_TOKEN }}
        run: |
          chmod +x git_update.sh
          ./git_update.sh